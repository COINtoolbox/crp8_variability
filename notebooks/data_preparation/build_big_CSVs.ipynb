{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d102fed-9c82-4346-8dac-3fe025836aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original by Emille Ishida, 31/08/2025 for CRP8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5bad143-f23b-438f-aefb-032dc17fe742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_catname(fname, dir_input):\n",
    "    \"\"\"Generate minimalist catalog name.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fname: str\n",
    "        complete path to csv file.\n",
    "    dir_input: str\n",
    "        input directory.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    catname: str\n",
    "        minimalist catalog name.\n",
    "    \"\"\"\n",
    "\n",
    "    cat_name = deepcopy(fname.replace(dir_input, ''))\n",
    "    cat_name = cat_name.replace('.fit.csv','')\n",
    "    \n",
    "    if '.csv' in cat_name:\n",
    "        cat_name = cat_name.replace('.csv', '')\n",
    "\n",
    "    return cat_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9572bc9-6816-4be3-9e48-af922e5a1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_ra_dec(columns, good_ra, good_dec, bad_ra, bad_dec):\n",
    "    \"\"\"Identify the columns containing RA, DEC information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    columns: list of str\n",
    "        list of all columns of a data frame.\n",
    "    good_ra: list of str\n",
    "        list of acceptable strings for RA.\n",
    "    good_dec: list of str\n",
    "        list of acceptable strings for DEC.\n",
    "    bad_ra: list of str\n",
    "        list of unnaceptable strings containing RA.\n",
    "    bad_dec: list of str\n",
    "        list of unnaceptable strings containing DE.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ra_name: str or None\n",
    "        Keyword identifying RA. Returns None if information not available.\n",
    "    dec_name: str or None\n",
    "        Keyword identifying DEC. Returns None if information not available.\n",
    "    \"\"\"\n",
    "    # initiatlize variables\n",
    "    ra_name = []\n",
    "    dec_name = []\n",
    "    ra_flag = False\n",
    "    dec_flag = False\n",
    "\n",
    "    for name in data.keys():\n",
    "        if name in good_ra:\n",
    "            ra_name.append(name)\n",
    "            ra_flag = True\n",
    "        elif name in good_dec:\n",
    "            dec_name.append(name)\n",
    "            dec_flag = True\n",
    "        elif not ra_flag and name not in bad_ra and 'ra' in name or 'RA' in name:\n",
    "            ra_name.append(name)\n",
    "        elif not dec_flag and name not in bad_dec and 'dec' in name or 'DEC' in name or 'DE' in name:\n",
    "            dec_name.append(name)\n",
    "\n",
    "    # check if the list is longer than it should be due to position of good keys\n",
    "    if len(ra_name) > 1 or len(dec_name) > 1:\n",
    "        for name in good_ra:\n",
    "            if name in ra_name:\n",
    "                ra_name = [name]\n",
    "        for name in  good_dec:\n",
    "            if name in dec_name:\n",
    "                dec_name = [name]\n",
    "\n",
    "    if len(ra_name) == 1 and len(dec_name) == 1:\n",
    "        return ra_name[0], dec_name[0]\n",
    "\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d268c494-4acc-4ea4-8fe3-7e1dd099b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_south(data, dec_lim=-30):\n",
    "    \"\"\"Remove objects that are too south given dec limit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pd.DataFrame\n",
    "        Data containing RA DEC info, it must have column names: ['ra','dec'].\n",
    "    dec_lim: float\n",
    "        Limiting value of DEC. Default is -30 (corresponding to ZTF coverage).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_use: pd.DataFrame\n",
    "        Data Frame containing only elements respecting given threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize list\n",
    "    ra_list = []\n",
    "    dec_list = []\n",
    "    row_flag = []\n",
    "    \n",
    "    for j in range(data.shape[0]):\n",
    "        if data['dec'].iloc[j] in ['deg'] or set(str(data['dec'].iloc[j])) ==  set('-'):\n",
    "            row_flag.append(False)\n",
    "            continue\n",
    "        elif ' ' in str(data['dec'].iloc[j]):\n",
    "            c = SkyCoord(data['ra'].iloc[j] + ' ' + data['dec'].iloc[j], unit=(u.hourangle, u.deg))\n",
    "            if c.dec.deg >= dec_lim:\n",
    "                ra_list.append(c.ra.deg)\n",
    "                dec_list.append(c.dec.deg)\n",
    "                row_flag.append(True)\n",
    "            else:\n",
    "               row_flag.append(False)\n",
    "        else:\n",
    "            if float(data['dec'].iloc[j]) >= dec_lim:\n",
    "                row_flag.append(True)\n",
    "            else:\n",
    "                row_flag.append(False)\n",
    "\n",
    "    row_flag = np.array(row_flag)\n",
    "    data_use = deepcopy(data[row_flag])\n",
    "\n",
    "    # only substitute if conversion was necessary\n",
    "    if len(ra_list) > 0:\n",
    "        data_use['ra'] = ra_list\n",
    "        data_use['dec'] = dec_list\n",
    "\n",
    "    return data_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7596d2b6-f573-4e01-a492-086e358250be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cat(data, dic_columns, dec_lim=-30):\n",
    "    \"\"\"\n",
    "    Identify RA DEC columns and remove objects not fulfilling DEC limit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pd.DataFrame\n",
    "        original content of csv file.\n",
    "    dec_lim: float\n",
    "        Limiting value of DEC. Default is -30 (corresponding to ZTF coverage).\n",
    "    dic_columns: dictionary\n",
    "        Identifies good and bad names for RA, DEC. Keywords must contain \n",
    "        ['good_ra', 'good_dec','bad_ra', 'bad_dec'].    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_use: pd.DataFrame or None\n",
    "        Data with RA DEC in degrees and within DEC limits.\n",
    "        If unable to identify RA DEC columns, returns None\n",
    "    \"\"\"\n",
    "   \n",
    "    # identify columns with RA, DEC info\n",
    "    ra_name, dec_name = identify_ra_dec(data.keys(), \n",
    "                                       dic_columns['good_ra'],\n",
    "                                       dic_columns['good_dec'], \n",
    "                                       dic_columns['bad_ra'],\n",
    "                                       dic_columns['bad_dec'])\n",
    "    \n",
    "    if ra_name is not None:\n",
    "        # rename columns to homogeneize nomenclature\n",
    "        data.rename(columns={ra_name:'ra', dec_name:'dec'}, inplace=True)\n",
    "\n",
    "        # remove things in the south\n",
    "        data_use = remove_south(data, dec_lim)\n",
    "\n",
    "        if data_use.shape[0] == 0:\n",
    "            data_use = -1\n",
    "    else:\n",
    "        data_use = None\n",
    "\n",
    "    return data_use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c99321ed-31de-4228-8cb9-19fe28c5508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_big_file(data, dic_columns, partition_size=95000):\n",
    "    \"\"\"\n",
    "    Process files with more than 100000 lines (limit of Fink xmatch).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pd.DataFrame\n",
    "        Full data from csv file\n",
    "    dic_columns: dictionary\n",
    "        Identifies good and bad names for RA, DEC. Keywords must contain \n",
    "        ['good_ra', 'good_dec','bad_ra', 'bad_dec']\n",
    "    partition_size: int\n",
    "        Maximum number of lines to be kept in one partition. Default is 95k.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_batches: dict \n",
    "        keys are the partition indexes and values pd.DataFrame with processed data.\n",
    "    \"\"\"\n",
    "    # identify columns with RA, DEC info\n",
    "    ra_name, dec_name = identify_ra_dec(data.keys(), dic_columns['good_ra'], \n",
    "                                            dic_columns['good_dec'], dic_columns['bad_ra'], \n",
    "                                            dic_columns['bad_dec'])\n",
    "\n",
    "    if ra_name is not None and dec_name is not None:\n",
    "\n",
    "        # determine the number of partitions\n",
    "        size_part = data.shape[0] // partition_size + 1\n",
    "        print('size_part=', size_part)\n",
    "\n",
    "        data_batches = {}\n",
    "\n",
    "        for j in range(size_part):\n",
    "           # slice data\n",
    "            data_part  = deepcopy(data.iloc[j*partition_size: (j + 1)*partition_size])\n",
    "            print('data_part.shape=', data_part.shape)\n",
    "\n",
    "            # define catalog name\n",
    "            cat_name = get_catname(fname, dir_input) \n",
    "        \n",
    "            # create identifier: catalog name + position in original file\n",
    "            if j < size_part - 1:\n",
    "                 data_part['cat_id'] = [cat_name + str('_') + str(item) \n",
    "                                       for item in range(j*partition_size,(j + 1)*partition_size)]\n",
    "            else:\n",
    "                data_part['cat_id'] = [cat_name + str('_') + str(item) \n",
    "                                       for item in range(j*partition_size,j*partition_size + data_part.shape[0])]\n",
    "\n",
    "            # create column with catalog name\n",
    "            data_part['catalog'] = cat_name\n",
    "\n",
    "            # process catalog\n",
    "            data_use = process_cat(data_part, dic_columns, dec_lim=dec_lim)\n",
    "            # store in dictionary\n",
    "            data_batches[j] = data_use[['cat_id','ra','dec','catalog']]\n",
    "            \n",
    "    else:\n",
    "        data_batches = None\n",
    "\n",
    "    return data_batches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475c8ad0-e74f-4c30-b9a8-f076052ce3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside big partition size!\n",
      "size_part= 3\n",
      "data_part.shape= (95000, 19)\n",
      "data_part.shape= (95000, 19)\n",
      "data_part.shape= (38055, 19)\n",
      "Saving big file, part: 1  file size: 95000\n",
      "Saving big file, part: 2  file size: 95000\n",
      "Saving big file, part: 3  file size: 38055\n",
      "Limit of objects achieved, file size: 91960\n",
      "Limit of objects achieved, file size: 84712\n",
      "Limit of objects achieved, file size: 73418\n",
      "Limit of objects achieved, file size: 92660\n",
      "Limit of objects achieved, file size: 62812\n",
      "Limit of objects achieved, file size: 71381\n",
      "Limit of objects achieved, file size: 93619\n",
      "Limit of objects achieved, file size: 58361\n",
      "Inside big partition size!\n",
      "size_part= 3\n",
      "data_part.shape= (95000, 19)\n",
      "data_part.shape= (95000, 19)\n",
      "data_part.shape= (4072, 19)\n",
      "Saving big file, part: 1  file size: 95000\n",
      "Saving big file, part: 2  file size: 95000\n",
      "Saving big file, part: 3  file size: 4072\n",
      "Limit of objects achieved, file size: 70581\n",
      "Limit of objects achieved, file size: 62405\n",
      "Inside big partition size!\n",
      "size_part= 2\n",
      "data_part.shape= (95000, 19)\n",
      "data_part.shape= (8786, 19)\n",
      "Saving big file, part: 1  file size: 95000\n",
      "Saving big file, part: 2  file size: 8786\n",
      "Limit of objects achieved, file size: 74955\n",
      "Limit of objects achieved, file size: 88767\n",
      "Limit of objects achieved, file size: 93349\n",
      "Inside big partition size!\n",
      "size_part= 2\n",
      "data_part.shape= (95000, 18)\n",
      "data_part.shape= (4999, 18)\n",
      "Saving big file, part: 1  file size: 63276\n",
      "Saving big file, part: 2  file size: 2864\n",
      "Limit of objects achieved, file size: 47641\n",
      "Limit of objects achieved, file size: 94170\n",
      "Inside big partition size!\n",
      "size_part= 14\n",
      "data_part.shape= (95000, 17)\n",
      "data_part.shape= (95000, 17)\n",
      "data_part.shape= (95000, 17)\n",
      "data_part.shape= (95000, 17)\n",
      "data_part.shape= (95000, 17)\n",
      "data_part.shape= (95000, 17)\n",
      "data_part.shape= (95000, 17)\n",
      "data_part.shape= (95000, 17)\n",
      "data_part.shape= (95000, 17)\n",
      "data_part.shape= (95000, 17)\n",
      "data_part.shape= (95000, 17)\n",
      "data_part.shape= (95000, 17)\n",
      "data_part.shape= (95000, 17)\n",
      "data_part.shape= (56929, 17)\n",
      "Saving big file, part: 1  file size: 63276\n",
      "Saving big file, part: 2  file size: 67156\n",
      "Saving big file, part: 3  file size: 67559\n",
      "Saving big file, part: 4  file size: 55217\n",
      "Saving big file, part: 5  file size: 63938\n",
      "Saving big file, part: 6  file size: 6691\n",
      "Saving big file, part: 7  file size: 52558\n",
      "Saving big file, part: 8  file size: 41525\n",
      "Saving big file, part: 9  file size: 48976\n",
      "Saving big file, part: 10  file size: 61723\n",
      "Saving big file, part: 11  file size: 74391\n",
      "Saving big file, part: 12  file size: 95000\n",
      "Saving big file, part: 13  file size: 67124\n",
      "Saving big file, part: 14  file size: 38861\n",
      "Limit of objects achieved, file size: 79513\n",
      "Inside big partition size!\n",
      "size_part= 13\n",
      "data_part.shape= (95000, 18)\n",
      "data_part.shape= (95000, 18)\n",
      "data_part.shape= (95000, 18)\n",
      "data_part.shape= (95000, 18)\n",
      "data_part.shape= (95000, 18)\n",
      "data_part.shape= (95000, 18)\n",
      "data_part.shape= (95000, 18)\n",
      "data_part.shape= (95000, 18)\n",
      "data_part.shape= (95000, 18)\n",
      "data_part.shape= (95000, 18)\n",
      "data_part.shape= (95000, 18)\n",
      "data_part.shape= (95000, 18)\n",
      "data_part.shape= (51929, 18)\n",
      "Saving big file, part: 1  file size: 69292\n",
      "Saving big file, part: 2  file size: 62601\n",
      "Saving big file, part: 3  file size: 60175\n",
      "Saving big file, part: 4  file size: 58938\n",
      "Saving big file, part: 5  file size: 6691\n",
      "Saving big file, part: 6  file size: 54828\n",
      "Saving big file, part: 7  file size: 41265\n",
      "Saving big file, part: 8  file size: 51966\n",
      "Saving big file, part: 9  file size: 60390\n",
      "Saving big file, part: 10  file size: 75724\n",
      "Saving big file, part: 11  file size: 95000\n",
      "Saving big file, part: 12  file size: 67124\n",
      "Saving big file, part: 13  file size: 33861\n",
      "Inside big partition size!\n",
      "size_part= 2\n",
      "data_part.shape= (95000, 19)\n",
      "data_part.shape= (49761, 19)\n",
      "Saving big file, part: 1  file size: 95000\n",
      "Saving big file, part: 2  file size: 49761\n",
      "Limit of objects achieved, file size: 94489\n",
      "Limit of objects achieved, file size: 49101\n",
      "Limit of objects achieved, file size: 69417\n",
      "Limit of objects achieved, file size: 93047\n",
      "Limit of objects achieved, file size: 76528\n",
      "Final batch! file size: 49087\n"
     ]
    }
   ],
   "source": [
    "rec_ini = 0                              # counter for accumulated number of processed objects\n",
    "list_to_pandas = []\n",
    "skiped_catalogs = []                     # could not recognize RA DEC columns\n",
    "processed_catalogs = []                  # processing successful\n",
    "south_catalogs = []                      # did not survive selection cuts\n",
    "\n",
    "dir_input = '/media3/CRP8/TDE/data/COIN2025_ClusterData/csv/galactic/'\n",
    "flist = glob.glob(dir_input + '*.csv')\n",
    "\n",
    "# directory to store big CSVs\n",
    "dir_out = '/media3/CRP8/TDE/data/COIN2025_ClusterData/csv_100k/galactic/'\n",
    "\n",
    "# limit of ZTF sky coverage\n",
    "dec_lim = -30\n",
    "\n",
    "dic_columns = {}\n",
    "dic_columns['good_ra'] = ['RA','ra', '_RA', 'RAJ2000', 'RA_ICRS', '_RA_icrs']\n",
    "dic_columns['good_dec'] = ['DEC','dec','_DE', 'DEJ2000', 'DE_ICRS', '_DE_icrs']\n",
    "dic_columns['bad_ra'] = ['ratio', 'galcen_radius', 'Separation', 'KRON_RADIUS_F850LP','KRON_RADIUS_F475W']\n",
    "dic_columns['bad_dec'] = ['DELTA_J2000_F850LP', 'DELTA_J2000_F475W']\n",
    "\n",
    "# partition size for big files\n",
    "partition_size = 95000\n",
    "\n",
    "# flag to identify if big file was processed\n",
    "big_file_done = False\n",
    "\n",
    "for fname in flist:\n",
    "    # read data\n",
    "    data = pd.read_csv(fname)\n",
    "    \n",
    "    # check if one file is too big\n",
    "    if data.shape[0] >= partition_size:\n",
    "        print('Inside big partition size!')\n",
    "        # process parts of the data\n",
    "        data_batches = process_big_file(data, dic_columns, partition_size=partition_size)\n",
    "\n",
    "        if data_batches is not None:\n",
    "            processed_catalogs.append(fname)\n",
    "            \n",
    "            # save big parts to file\n",
    "            for part in sorted(list(data_batches.keys())):\n",
    "                cat_name = data_batches[part].iloc[0]['catalog']\n",
    "                data_batches[part].to_csv(dir_out + 'batch_' + cat_name + '_' + str(part*partition_size) + \\\n",
    "                                          '_' + str((part + 1)*partition_size) + '.csv', index=False)\n",
    "                print('Saving big file, part:', part + 1, ' file size:', data_batches[part].shape[0])\n",
    "\n",
    "        else:\n",
    "            skiped_catalogs.append(fname)\n",
    "            print('Could not recognize RA DEC for big file.')\n",
    "\n",
    "        big_file_done = True\n",
    "            \n",
    "    # if the new file will be too big, save current records\n",
    "    elif not big_file_done:\n",
    "        if data.shape[0] + rec_ini >= partition_size:        \n",
    "            data_batch = pd.concat(list_to_pandas, ignore_index=True)\n",
    "            data_batch.to_csv(dir_out + 'batch_' + cat_name + '_' + \\\n",
    "                              str(rec_ini - data_use.shape[0]) + '_' + str(rec_ini) + '.csv', \n",
    "                                            index=False)   \n",
    "            print('Limit of objects achieved, file size:', data_batch.shape[0])\n",
    "      \n",
    "            list_to_pandas = []      # reset storage\n",
    "            rec_ini = 0              # reset counter\n",
    "\n",
    "        # get minimalist catalog name\n",
    "        cat_name = get_catname(fname, dir_input)\n",
    "        \n",
    "        # create identifier: catalog name + position in original file\n",
    "        data['cat_id'] = [cat_name + str('_') + str(item) for item in range(data.shape[0])]\n",
    "\n",
    "        data_use = process_cat(data, dic_columns, dec_lim=dec_lim)\n",
    "\n",
    "        if data_use is not None and isinstance(data_use, pd.DataFrame):            \n",
    "            # update count\n",
    "            rec_ini = rec_ini + data_use.shape[0]\n",
    "            \n",
    "            # append to already processed data\n",
    "            list_to_pandas.append(data_use[['cat_id','ra', 'dec']])\n",
    "            processed_catalogs.append(fname)\n",
    "       \n",
    "        else:\n",
    "            if data_use != -1:\n",
    "                skiped_catalogs.append(fname)\n",
    "            else:\n",
    "                south_catalogs.append(fname)\n",
    "\n",
    "        # save to file if it is the last catalog\n",
    "        if fname == flist[-1]:\n",
    "            data_batch = pd.concat(list_to_pandas, ignore_index=True)\n",
    "            data_batch.to_csv(dir_out + 'batch_' + cat_name + '_' + str(rec_ini - data_use.shape[0]) + '_' + str(rec_ini) + '.csv', \n",
    "                                            index=False)   \n",
    "            print('Final batch! file size:', data_batch.shape[0])\n",
    "\n",
    "    big_file_done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df896102-259e-4292-a68e-2788fd473bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed catalogs:  172\n",
      "Number of catalogs not surviving selection cuts:  0\n",
      "Number of catalogs without RA DEC columns:  0\n",
      "Total number of accounted catalogs:  172\n",
      "Total number of catalogs:  172\n"
     ]
    }
   ],
   "source": [
    "print('Number of processed catalogs: ', len(processed_catalogs))\n",
    "print('Number of catalogs not surviving selection cuts: ', len(south_catalogs))\n",
    "print('Number of catalogs without RA DEC columns: ', len(skiped_catalogs))\n",
    "print('Total number of accounted catalogs: ', len(processed_catalogs)+len(south_catalogs)+len(skiped_catalogs))\n",
    "\n",
    "print('Total number of catalogs: ', len(flist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2957d9ca-f786-4a9e-9358-3e7843806faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in skiped_catalogs:\n",
    "    print(fname.replace('/media3/CRP8/TDE/data/COIN2025_ClusterData/csv/extragalactic/',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ae43e-ad7d-4f8f-88c2-9099f5f183b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
