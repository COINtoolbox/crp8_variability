{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d102fed-9c82-4346-8dac-3fe025836aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original by Emille Ishida, 31/08/2025 for CRP8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d6a651-7448-4d11-b11d-f21840321c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = '/media3/CRP8/TDE/data/COIN2025_ClusterData/csv/extragalactic/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3172553-7b3e-447b-99e6-d5d874a3aa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = glob.glob(dirname + '*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef8ac695-350a-4a16-9f4e-7f1fc724b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to store big CSVs\n",
    "dir_out = '/media3/CRP8/TDE/data/COIN2025_ClusterData/csv_100k/extragalactic/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b369176-f2f8-42aa-800c-e6b9e2d93d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit of ZTF sky coverage\n",
    "dec_lim = -30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5bad143-f23b-438f-aefb-032dc17fe742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_catname(fname, dir_input):\n",
    "    \"\"\"Generate minimalist catalog name.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fname: str\n",
    "        complete path to csv file.\n",
    "    dir_input: str\n",
    "        input directory.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    catname: str\n",
    "        minimalist catalog name.\n",
    "    \"\"\"\n",
    "\n",
    "    cat_name = deepcopy(fname.replace(dir_input, ''))\n",
    "    cat_name = cat_name.replace('.fit.csv','')\n",
    "    \n",
    "    if '.csv' in cat_name:\n",
    "        cat_name = cat_name.replace('.csv', '')\n",
    "\n",
    "    return cat_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9572bc9-6816-4be3-9e48-af922e5a1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_ra_dec(columns, good_ra, good_dec, bad_ra, bad_dec):\n",
    "    \"\"\"Identify the columns containing RA, DEC information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    columns: list of str\n",
    "        list of all columns of a data frame.\n",
    "    good_ra: list of str\n",
    "        list of acceptable strings for RA.\n",
    "    good_dec: list of str\n",
    "        list of acceptable strings for DEC.\n",
    "    bad_ra: list of str\n",
    "        list of unnaceptable strings containing RA.\n",
    "    bad_dec: list of str\n",
    "        list of unnaceptable strings containing DE.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ra_name: str or None\n",
    "        Keyword identifying RA. Returns None if information not available.\n",
    "    dec_name: str or None\n",
    "        Keyword identifying DEC. Returns None if information not available.\n",
    "    \"\"\"\n",
    "    # initiatlize variables\n",
    "    ra_name = []\n",
    "    dec_name = []\n",
    "    ra_flag = False\n",
    "    dec_flag = False\n",
    "\n",
    "    for name in data.keys():\n",
    "        if name in good_ra:\n",
    "            ra_name.append(name)\n",
    "            ra_flag = True\n",
    "        elif name in good_dec:\n",
    "            dec_name.append(name)\n",
    "            dec_flag = True\n",
    "        elif not ra_flag and name not in bad_ra and 'ra' in name or 'RA' in name:\n",
    "            ra_name.append(name)\n",
    "        elif not dec_flag and name not in bad_dec and 'dec' in name or 'DEC' in name or 'DE' in name:\n",
    "            dec_name.append(name)\n",
    "\n",
    "    # check if the list is longer than it should be due to position of good keys\n",
    "    if len(ra_name) > 1 or len(dec_name) > 1:\n",
    "        for name in good_ra:\n",
    "            if name in ra_name:\n",
    "                ra_name = [name]\n",
    "        for name in  good_dec:\n",
    "            if name in dec_name:\n",
    "                dec_name = [name]\n",
    "\n",
    "    if len(ra_name) == 1 and len(dec_name) == 1:\n",
    "        return ra_name[0], dec_name[0]\n",
    "\n",
    "    else:\n",
    "        return None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d268c494-4acc-4ea4-8fe3-7e1dd099b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_south(data, dec_lim=-30):\n",
    "    \"\"\"Remove objects that are too south given dec limit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pd.DataFrame\n",
    "        Data containing RA DEC info, it must have column names: ['ra','dec'].\n",
    "    dec_lim: float\n",
    "        Limiting value of DEC. Default is -30 (corresponding to ZTF coverage).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_use: pd.DataFrame\n",
    "        Data Frame containing only elements respecting given threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize list\n",
    "    ra_list = []\n",
    "    dec_list = []\n",
    "    row_flag = []\n",
    "    \n",
    "    for j in range(data.shape[0]):\n",
    "        if data['dec'].iloc[j] in ['deg'] or set(str(data['dec'].iloc[j])) ==  set('-'):\n",
    "            row_flag.append(False)\n",
    "            continue\n",
    "        elif ' ' in str(data['dec'].iloc[j]):\n",
    "            c = SkyCoord(data['ra'].iloc[j] + ' ' + data['dec'].iloc[j], unit=(u.hourangle, u.deg))\n",
    "            if c.dec.deg >= dec_lim:\n",
    "                ra_list.append(c.ra.deg)\n",
    "                dec_list.append(c.dec.deg)\n",
    "                row_flag.append(True)\n",
    "            else:\n",
    "               row_flag.append(False)\n",
    "        else:\n",
    "            if float(data['dec'].iloc[j]) >= dec_lim:\n",
    "                row_flag.append(True)\n",
    "            else:\n",
    "                row_flag.append(False)\n",
    "\n",
    "    row_flag = np.array(row_flag)\n",
    "    data_use = deepcopy(data[row_flag])\n",
    "\n",
    "    # only substitute if conversion was necessary\n",
    "    if len(ra_list) > 0:\n",
    "        data_use['ra'] = ra_list\n",
    "        data_use['dec'] = dec_list\n",
    "\n",
    "    return data_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7596d2b6-f573-4e01-a492-086e358250be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cat(data, dic_columns, dec_lim=-30):\n",
    "    \"\"\"\n",
    "    Identify RA DEC columns and remove objects not fulfilling DEC limit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pd.DataFrame\n",
    "        original content of csv file.\n",
    "    dec_lim: float\n",
    "        Limiting value of DEC. Default is -30 (corresponding to ZTF coverage).\n",
    "    dic_columns: dictionary\n",
    "        Identifies good and bad names for RA, DEC. Keywords must contain \n",
    "        ['good_ra', 'good_dec','bad_ra', 'bad_dec'].    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_use: pd.DataFrame or None\n",
    "        Data with RA DEC in degrees and within DEC limits.\n",
    "        If unable to identify RA DEC columns, returns None\n",
    "    \"\"\"\n",
    "   \n",
    "    # identify columns with RA, DEC info\n",
    "    ra_name, dec_name = identify_ra_dec(data.keys(), \n",
    "                                       dic_columns['good_ra'],\n",
    "                                       dic_columns['good_dec'], \n",
    "                                       dic_columns['bad_ra'],\n",
    "                                       dic_columns['bad_dec'])\n",
    "    \n",
    "    if ra_name is not None:\n",
    "        # rename columns to homogeneize nomenclature\n",
    "        data.rename(columns={ra_name:'ra', dec_name:'dec'}, inplace=True)\n",
    "\n",
    "        # remove things in the south\n",
    "        data_use = remove_south(data, dec_lim)\n",
    "\n",
    "        if data_use.shape[0] == 0:\n",
    "            data_use = -1\n",
    "    else:\n",
    "        data_use = None\n",
    "\n",
    "    return data_use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c99321ed-31de-4228-8cb9-19fe28c5508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_big_file(data, dic_columns, partition_size=95000):\n",
    "    \"\"\"\n",
    "    Process files with more than 100000 lines (limit of Fink xmatch).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pd.DataFrame\n",
    "        Full data from csv file\n",
    "    dic_columns: dictionary\n",
    "        Identifies good and bad names for RA, DEC. Keywords must contain \n",
    "        ['good_ra', 'good_dec','bad_ra', 'bad_dec']\n",
    "    partition_size: int\n",
    "        Maximum number of lines to be kept in one partition. Default is 95k.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_batches: dict \n",
    "        keys are the partition indexes and values pd.DataFrame with processed data.\n",
    "    \"\"\"\n",
    "    # identify columns with RA, DEC info\n",
    "    ra_name, dec_name = identify_ra_dec(data.keys(), dic_columns['good_ra'], \n",
    "                                            dic_columns['good_dec'], dic_columns['bad_ra'], \n",
    "                                            dic_columns['bad_dec'])\n",
    "\n",
    "    if ra_name is not None and dec_name is not None:\n",
    "\n",
    "        # determine the number of partitions\n",
    "        size_part = data.shape[0] // partition_size + 1\n",
    "        print('size_part=', size_part)\n",
    "\n",
    "        data_batches = {}\n",
    "\n",
    "        for j in range(size_part):\n",
    "           # slice data\n",
    "            data_part  = deepcopy(data.iloc[j*partition_size: (j + 1)*partition_size])\n",
    "            print('data_part.shape=', data_part.shape)\n",
    "\n",
    "            # define catalog name\n",
    "            cat_name = get_catname(fname, dir_input) \n",
    "        \n",
    "            # create identifier: catalog name + position in original file\n",
    "            if j < size_part - 1:\n",
    "                 data_part['cat_id'] = [cat_name + str('_') + str(item) \n",
    "                                       for item in range(j*partition_size,(j + 1)*partition_size)]\n",
    "            else:\n",
    "                data_part['cat_id'] = [cat_name + str('_') + str(item) \n",
    "                                       for item in range(j*partition_size,j*partition_size + data_part.shape[0])]\n",
    "\n",
    "            # create column with catalog name\n",
    "            data_part['catalog'] = cat_name\n",
    "\n",
    "            # process catalog\n",
    "            data_use = process_cat(data_part, dic_columns, dec_lim=dec_lim)\n",
    "            store in dictionary\n",
    "            data_batches[j] = data_use\n",
    "            \n",
    "    else:\n",
    "        data_batches = None\n",
    "\n",
    "    return data_batches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "475c8ad0-e74f-4c30-b9a8-f076052ce3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGC3379_F850LP_F475W.cat.r.absMag.color.apcor.fGC skipped!\n",
      "NGC4278_F850LP_F475W.cat.r.absMag.color.apcor.GC skipped!\n",
      "woodley2010a skipped!\n",
      "NGC4278_F850LP_F475W.cat.r.absMag.color.apcor.fGC skipped!\n",
      "woodley2010b skipped!\n",
      "maybhate09 skipped!\n",
      "Strader12UCDs skipped!\n",
      "goudfrooij06 skipped!\n",
      "goudrooij12 skipped!\n",
      "Degraaf skipped!\n",
      "NGC3607_sources skipped!\n",
      " Kontizas90 skipped!\n",
      "NGC4594_F625W_F435W.cat.r.absMag.color.apcor.GC skipped!\n",
      "NGC4472_F850LP_F475W.cat.r.absMag.color.apcor.fGC skipped!\n",
      "Limit of objects achieved, file size: 94720\n",
      "NGC4472_F850LP_F475W.cat.r.absMag.color.apcor.GC skipped!\n",
      "NGC4594_F625W_F435W.cat.r.absMag.color.apcor.fGC skipped!\n",
      "NGC3607_Gcc skipped!\n",
      "su2022 skipped!\n",
      "NGC7331_Gcc skipped!\n",
      "Could not recognize RA DEC for big file.\n",
      "barmby_m31_phot skipped!\n",
      "ACSFCS skipped!\n",
      "barmby_m31_spec skipped!\n",
      "NGC3379_F850LP_F475W.cat.r.absMag.color.apcor.GC skipped!\n",
      "chattopadhyay2009 skipped!\n",
      "bassino skipped!\n",
      "Limit of objects achieved, file size: 31572\n"
     ]
    }
   ],
   "source": [
    "rec_ini = 0\n",
    "list_to_pandas = []\n",
    "skiped_catalogs = []\n",
    "processed_catalogs = []\n",
    "cat_end = None\n",
    "\n",
    "dir_input = '/media3/CRP8/TDE/data/COIN2025_ClusterData/csv/extragalactic/'\n",
    "\n",
    "dic_columns = {}\n",
    "dic_columns['good_ra'] = ['RA','ra', '_RA', 'RAJ2000', 'RA_ICRS', '_RA_icrs']\n",
    "dic_columns['good_dec'] = ['DEC','dec','_DE', 'DEJ2000', 'DE_ICRS', '_DE_icrs']\n",
    "dic_columns['bad_ra'] = ['ratio', 'galcen_radius', 'Separation', 'KRON_RADIUS_F850LP','KRON_RADIUS_F475W']\n",
    "dic_columns['bad_dec'] = ['DELTA_J2000_F850LP', 'DELTA_J2000_F475W']\n",
    "\n",
    "# partition size for big files\n",
    "partition_size = 95000\n",
    "\n",
    "# flag to identify if big file was processed\n",
    "big_file_done = False\n",
    "\n",
    "for fname in flist:\n",
    "    # read data\n",
    "    data = pd.read_csv(fname)\n",
    "\n",
    "    # check if one file is too big\n",
    "    if data.shape[0] >= partition_size:\n",
    "        # process parts of the data\n",
    "        data_batches = process_big_file(data, dic_columns, partition_size=partition_size)\n",
    "\n",
    "        if data_batches is not None:\n",
    "            # save big parts to file\n",
    "            for part in sorted(list(data_batches.keys()))[:-1]:\n",
    "                cat_name = data_batches[part].iloc[0]['catalog']\n",
    "                data_batches[part].to_csv(dir_out + 'batch_' + cat_name + '_' + str(part*partition_size) + \\\n",
    "                                          '_' + str((part + 1)*partition_size) + '.csv', index=False)\n",
    "                print('Saving big file, part:', part + 1, ' file size:', data_batches[part].shape[0])\n",
    "\n",
    "            # identify last part\n",
    "            part = sorted(list(data_batches.keys()))[-1]\n",
    "            if data_batches[part].shape[0] + rec_ini < partition_size:\n",
    "                # update count\n",
    "                rec_ini = rec_ini + data_batches[part].shape[0]\n",
    "\n",
    "                # add to list of previously processed data\n",
    "                list_to_pandas.append(data_batches[part].shape[0])\n",
    "\n",
    "            else:\n",
    "                # save to file\n",
    "                cat_name = data_batches[part].iloc[0]['catalog']\n",
    "                data_batches[part].to_csv(dir_out + 'batch_' + cat_name + '_' + str(part*partition_size) + \\\n",
    "                                          '_' + str(part*partition_size + data_batches[part].shape[0]) + '.csv', index=False)\n",
    "                print('Saving big file, part:', part, ' file size:', data_batches[part].shape[0])\n",
    "\n",
    "        else:\n",
    "            print(cat_name, 'skipped!')\n",
    "            skiped_catalogs.append(fname)\n",
    "            print('Could not recognize RA DEC for big file.')\n",
    "\n",
    "        big_file_done = True\n",
    "            \n",
    "    # if the new file will be too big, save current records\n",
    "    elif not big_file_done and data.shape[0] + rec_ini > partition_size or fname == flist[-1]:\n",
    "        data_batch = pd.concat(list_to_pandas, ignore_index=True)\n",
    "        data_batch.to_csv(dir_out + 'batch_' + cat_name + '_' + str(rec_ini - data_use.shape[0]) + '_' + str(rec_ini) + '.csv', \n",
    "                                            index=False)   \n",
    "        print('Limit of objects achieved, file size:', data_batch.shape[0])\n",
    "        if fname != flist[-1]:\n",
    "            list_to_pandas = []\n",
    "        rec_ini = 0\n",
    "\n",
    "    elif data.shape[0] + rec_ini < partition_size and not big_file_done: \n",
    "        # get minimalist catalog name\n",
    "        cat_name = get_catname(fname, dir_input)\n",
    "        \n",
    "        # create identifier: catalog name + position in original file\n",
    "        data['cat_id'] = [cat_name + str('_') + str(item) for item in range(data.shape[0])]\n",
    "\n",
    "        data_use = process_cat(data, dic_columns, dec_lim=dec_lim)\n",
    "\n",
    "        if data_use is not None and isinstance(data_use, pd.DataFrame):            \n",
    "            # update count\n",
    "            rec_ini = rec_ini + data_use.shape[0]\n",
    "            \n",
    "            # append to already processed data\n",
    "            list_to_pandas.append(data_use[['cat_id','ra', 'dec']])\n",
    "       \n",
    "        else:\n",
    "            print(cat_name, 'skipped!')\n",
    "\n",
    "            if data_use != -1:\n",
    "                skiped_catalogs.append(fname)\n",
    "\n",
    "    big_file_done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df896102-259e-4292-a68e-2788fd473bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(skiped_catalogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2957d9ca-f786-4a9e-9358-3e7843806faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media3/CRP8/TDE/data/COIN2025_ClusterData/csv/extragalactic/NGC3379_F850LP_F475W.cat.r.absMag.color.apcor.fGC.csv',\n",
       " '/media3/CRP8/TDE/data/COIN2025_ClusterData/csv/extragalactic/NGC4278_F850LP_F475W.cat.r.absMag.color.apcor.GC.csv',\n",
       " '/media3/CRP8/TDE/data/COIN2025_ClusterData/csv/extragalactic/NGC4278_F850LP_F475W.cat.r.absMag.color.apcor.fGC.csv',\n",
       " '/media3/CRP8/TDE/data/COIN2025_ClusterData/csv/extragalactic/Strader12UCDs.fit.csv',\n",
       " '/media3/CRP8/TDE/data/COIN2025_ClusterData/csv/extragalactic/NGC4594_F625W_F435W.cat.r.absMag.color.apcor.GC.csv',\n",
       " '/media3/CRP8/TDE/data/COIN2025_ClusterData/csv/extragalactic/NGC4472_F850LP_F475W.cat.r.absMag.color.apcor.fGC.csv',\n",
       " '/media3/CRP8/TDE/data/COIN2025_ClusterData/csv/extragalactic/NGC4472_F850LP_F475W.cat.r.absMag.color.apcor.GC.csv',\n",
       " '/media3/CRP8/TDE/data/COIN2025_ClusterData/csv/extragalactic/NGC4594_F625W_F435W.cat.r.absMag.color.apcor.fGC.csv',\n",
       " '/media3/CRP8/TDE/data/COIN2025_ClusterData/csv/extragalactic/su2022.fit.csv',\n",
       " '/media3/CRP8/TDE/data/COIN2025_ClusterData/csv/extragalactic/ACSVCS.csv',\n",
       " '/media3/CRP8/TDE/data/COIN2025_ClusterData/csv/extragalactic/barmby_m31_phot.csv',\n",
       " '/media3/CRP8/TDE/data/COIN2025_ClusterData/csv/extragalactic/barmby_m31_spec.csv',\n",
       " '/media3/CRP8/TDE/data/COIN2025_ClusterData/csv/extragalactic/NGC3379_F850LP_F475W.cat.r.absMag.color.apcor.GC.csv']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for fname in skiped_catalogs:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ae43e-ad7d-4f8f-88c2-9099f5f183b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
